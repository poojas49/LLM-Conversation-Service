ollama {
  host = "http://ollama:11434"
  host = ${?OLLAMA_HOST}
  model = "llama2:latest"
  model = ${?OLLAMA_MODEL}
  request-timeout-seconds = 500
}

service {
  host = "localhost"
  host = ${?SERVICE_HOST}
  port = 8080
  port = ${?SERVICE_PORT}
}

conversation {
  max-turns = 5
  timeout-minutes = 30
}

cloud-service {
  temperature = 0.7
  max-tokens = 150
  request-timeout-seconds = 30
}

server {
  host = "0.0.0.0"
  host = ${?SERVER_HOST}
  port = 8081
  port = ${?SERVER_PORT}
  termination-timeout-seconds = 10
}