ollama {
  host = "http://localhost:11434"
  model = "llama2:latest"
  request-timeout-seconds = 500
}

service {
  host = "localhost"
  port = 8080
}

conversation {
  max-turns = 5
  timeout-minutes = 30
}

cloud-service {
  temperature = 0.7
  max-tokens = 150
  request-timeout-seconds = 30
}

server {
  host = "0.0.0.0"
  port = 8081
  termination-timeout-seconds = 10
}